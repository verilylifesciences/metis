{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Verily Life Sciences LLC\n",
    "\n",
    "Use of this source code is governed by a BSD-style\n",
    "license that can be found in the LICENSE file or at\n",
    "https://developers.google.com/open-source/licenses/bsd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial Specification Demo\n",
    "\n",
    "The first step to use the Metis tool is to specify your trial.\n",
    "\n",
    "All data in the Metis tool is stored in [xarray.DataArray](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.html) datasets. This is a [convenient datastructure](http://xarray.pydata.org/en/stable/why-xarray.html) for storing multidimensional arrays with different labels, coordinates or attributes.    You don't need to have any expertise with xr.Datasets to use the Metis toolkit. The goal of this notebook is to walk you through the construction of the dataset that contains the specification of your trial.\n",
    "\n",
    "This notebook has several sections:\n",
    "1. **Define the Trial**. In this section you will load all aspects of your trial, including the trial sites, the expected recruitment demographics for each trial site (e.g. from a census) as well as the rules for how the trial will be carried out.\n",
    "2. **Load Incidence Forecasts**.  In this section you will load forecasts for covid incidence at the locations of your trial. We highly recommend using forecasts that are as local as possible for the sites of the trial. There is significant variation in covid incidence among counties in the same state, and taking the state (province) average can be highly misleading. Here we include code to preload forecasts for county level forecasts from the US Center for Disease Control. The trial planner should include whatever forecasts they find most compelling. \n",
    "3. **Simulate the Trial** Given the incidence forecasts and the trial rules, the third section will simulate the trial.\n",
    "4. **Optimize the Trial** Given the parameters of the trial within our control, the next section asks whether we can  set those parameters to make the trial meet our objective criteria, for example most likely to succeed or to succeed as quickly as possible. We have written a set of optimization routines for optimizing different types of trials.\n",
    "\n",
    "We write out different trial plans, which you can then examine interactively in the second notebook in the Metis Toolbox. That notebook lets you visualize how the trial is proceeding at a per site level and experiment with what will happen when you turn up or down different sites.\n",
    "\n",
    "If you have questions about how to implement these steps for your clinical trial, or there are variations in the trial specification that are not captured with this framework, please contact metis@projectbaseline.com for additional help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('ticks')\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import xarray as xr\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# metis imports\n",
    "import sys\n",
    "import os\n",
    "repo_dir = os.path.dirname(os.getcwd())\n",
    "def add_if_needed(path):\n",
    "  if path not in sys.path:\n",
    "    sys.path.append(path)\n",
    "add_if_needed(os.path.join(repo_dir, 'py'))\n",
    "\n",
    "from metis import io as metis_io\n",
    "from metis import util\n",
    "from metis import optimization\n",
    "from metis import sim\n",
    "from metis import sim_scenarios\n",
    "from metis import public_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_participants(participants):\n",
    "  time = participants.time.values\n",
    "  util.sum_all_but_dims(['time'], participants).cumsum('time').plot()\n",
    "  plt.title('Participants recruited (both control and treatment arm)')\n",
    "  plt.xlim(time[0], time[-1])\n",
    "  plt.ylim(bottom=0)\n",
    "  plt.show()\n",
    "\n",
    "def plot_events(events):\n",
    "  time = events.time.values\n",
    "  events.cumsum('time').plot.line(x='time', color='k', alpha=.02, add_legend=False)\n",
    "  for analysis, num_events in c.needed_control_arm_events.to_series().items():\n",
    "    plt.axhline(num_events, linestyle='--')\n",
    "    plt.text(time[0], num_events, analysis, ha='left', va='bottom')\n",
    "  plt.ylim(0, 120)\n",
    "  plt.xlim(time[0], time[-1])\n",
    "  plt.title(f'Control arm events\\n{events.scenario.size} simulated scenarios')\n",
    "  plt.show()\n",
    "\n",
    "def plot_success(c, events):\n",
    "  time = c.time.values\n",
    "  success_day = xr.DataArray(util.success_day(c.needed_control_arm_events, events),\n",
    "                             coords=(events.scenario, c.analysis))\n",
    "\n",
    "  fig, axes = plt.subplots(c.analysis.size, 1, sharex=True)\n",
    "  step = max(1, int(np.timedelta64(3, 'D') / (time[1] - time[0])))\n",
    "  bins = mpl.units.registry[np.datetime64].convert(time[::step], None, None)\n",
    "\n",
    "  for analysis, ax in zip(c.analysis.values, axes):\n",
    "    success_days = success_day.sel(analysis=analysis).values\n",
    "    np.where(np.isnat(success_days), np.datetime64('2050-06-01'), success_days)\n",
    "    ax.hist(success_days, bins=bins, density=True)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    # subtract time[0] to make into timedelta64s so that we can take a mean/median\n",
    "    median = np.median(success_days - time[0]) + time[0]\n",
    "    median = pd.to_datetime(median).date()\n",
    "    ax.axvline(median, color='r')\n",
    "    ax.text(time[0], 0, f'{analysis}\\n{median} median', ha='left', va='bottom')\n",
    "\n",
    "  plt.xlabel('Date when sufficient statistical power is achieved')\n",
    "  plt.xlim(time[0], time[-1])\n",
    "  plt.xticks(rotation=35)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define the trial\n",
    "\n",
    "##  Choose the sites\n",
    "A trial specification consists a list of sites, together with various properties of the sites. \n",
    "\n",
    "This information is loaded from the file `demo_data/site_list1.csv`. Each row of this file contains the name of a site, as well as the detailed information about the trial.  In this illustrative example, we pick sites in real US counties. Each column contains the following information:\n",
    "\n",
    "* `opencovid_key` . This is a key that specifies location within [COVID-19 Open Data](https://github.com/GoogleCloudPlatform/covid-19-open-data). It is required by this schema because it is the way we join the incidence forecasts to the site locations.  \n",
    "* `capacity`, the number of participants the site can recruit each week, including both control arm and treatment arms. For simplicity, we assume this is constant over time, but variable recruitment rates are also supported. (See the construction of the `site_capacity` array below).\n",
    "* `start_date`. This is the first date on which the site can recruit participants.\n",
    "* The proportion of the population in various demographic categories. For this example, we consider categories for age (`over_60`), ethnicity (`black`, `hisp_lat`), and comorbidities (`smokers`, `diabetes`, `obese`). **Here we just fill in demographic information with random numbers.** We assume different categories are independent, but the data structure supports complex beliefs about how different categories intersect, how much each site can enrich for different categories, and different infection risks for different categories. These are represented in the factors  `population_fraction`, `participant_fraction`, `incidence_scaler`, and `incidence_to_event_factor` below.  In a practical situation, we recommend that the trial planner uses accurate estimates of the populations for the different sites they are drawing from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_df = pd.read_csv('demo_data/site_list1.csv', index_col=0)\n",
    "site_df.index.name = 'location'\n",
    "site_df['start_date'] = pd.to_datetime(site_df['start_date'])\n",
    "display(site_df)\n",
    "\n",
    "# Add in information we have about each county.\n",
    "site_df = pd.concat([site_df, public_data.us_county_data().loc[site_df.opencovid_key].set_index(site_df.index)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose trial parameters\n",
    "The trial requires a number of parameters that have to be specified to be able to simulate what will happen in the trial: These include:\n",
    "\n",
    "\n",
    "* `trial_size_cap`: the maximum number of participants in the trial (includes both control and treatment arms)\n",
    "* `start_day` and `end_day`: the boundaries of the time period we will simulate.\n",
    "* `proportion_control_arm`: what proportion of participants are in the control arm. It's assumed that the control arm is as uniformly distributed across locations and time (e.g. at each location on each day, half of the recruited participants are assigned to the control arm).\n",
    "* `needed_control_arm_events`: the number of events required in the *control* arm of the trial at various intermediate analysis points. For this example we assume intermediate analyses which would demonstrate a vaccine efficacy of about 55%, 65%, 75%, 85%, or 95%.\n",
    "* `observation_delay`: how long after a participant is recruited before they contribute an event. This is measured in the same time units as your incidence forecasts. Here we assume 28 days.\n",
    "* `site_capacity` and `site_activation`: the number of participants each site could recruit *if* it were activated, and whether each site is activated at any given time. Here we assume each site as a constant weekly capacity, but time dependence can be included (e.g. to model ramp up of recruitment).\n",
    "* `population_fraction`, `participant_fraction`, and `incidence_scaler`: the proportion of the general population and the proportion of participants who fall into different demographic categories at each location, and the infection risk factor for each category. These three are required to translate an overall incidence forecast for the population into the incidence forecast for your control arm.\n",
    "* `incidence_to_event_factor`: what proportion of infections lead to a clinical event. We assume a constant 0.6, but you can specify different values for different demographic categories.\n",
    "\n",
    "These factors are specified in the datastructure below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_day = np.datetime64('2020-11-01')\n",
    "end_day = np.datetime64('2021-03-01')\n",
    "time_resolution = np.timedelta64(1, 'D')\n",
    "\n",
    "time = np.arange(start_day, end_day + time_resolution, time_resolution)\n",
    "c = xr.Dataset(coords=dict(time=time))\n",
    "c['proportion_control_arm'] = 0.5\n",
    "\n",
    "# Assume some intermediate analyses.\n",
    "frac_control = float(c.proportion_control_arm)\n",
    "efficacy = np.array([.55, .65, .75, .85, .95])\n",
    "ctrl_events = util.needed_control_arm_events(efficacy, frac_control)\n",
    "vaccine_events = (1 - efficacy) * ctrl_events * (1 - frac_control) / frac_control\n",
    "ctrl_events, vaccine_events = np.round(ctrl_events), np.round(vaccine_events)\n",
    "efficacy = 1 - (vaccine_events / ctrl_events)\n",
    "total_events = ctrl_events + vaccine_events\n",
    "analysis_names = [\n",
    "    f'{int(t)} total events @{int(100 * e)}% VE' for t, e in zip(total_events, efficacy)\n",
    "]\n",
    "c['needed_control_arm_events'] = xr.DataArray(\n",
    "    ctrl_events, dims=('analysis',)).assign_coords(analysis=analysis_names)\n",
    "\n",
    "c['recruitment_type'] = 'default'\n",
    "c['observation_delay'] = int(np.timedelta64(28, 'D') / time_resolution)  # 28 days\n",
    "c['trial_size_cap'] = 30000\n",
    "\n",
    "# convert weekly capacity to capacity per time step\n",
    "site_capacity = site_df.capacity.to_xarray() * time_resolution / np.timedelta64(7, 'D')\n",
    "site_capacity = site_capacity.broadcast_like(c.time).astype('float')\n",
    "# Can't recruit before the activation date\n",
    "activation_date = site_df.start_date.to_xarray()\n",
    "for l in activation_date.location.values:\n",
    "  date = activation_date.loc[l]\n",
    "  site_capacity.loc[site_capacity.time < date, l] = 0.0\n",
    "c['site_capacity'] = site_capacity.transpose('location', 'time')\n",
    "\n",
    "c['site_activation'] = xr.ones_like(c.site_capacity)\n",
    "\n",
    "# For the sake of simplicity, this code assumes black and hisp_lat are\n",
    "# non-overlapping, and that obese/smokers/diabetes are non-overlapping.\n",
    "frac_and_scalar = util.fraction_and_incidence_scaler\n",
    "fraction_scalers = [\n",
    "    frac_and_scalar(site_df, 'age', ['over_60'], [1], 'under_60'),\n",
    "    frac_and_scalar(site_df, 'ethnicity', ['black', 'hisp_lat'], [1, 1],\n",
    "                    'other'),\n",
    "    frac_and_scalar(site_df, 'comorbidity', ['smokers', 'diabetes', 'obese'],\n",
    "                    [1, 1, 1], 'none')\n",
    "]\n",
    "fractions, incidence_scalers = zip(*fraction_scalers)\n",
    "\n",
    "# We assume that different categories are independent (e.g. the proportion of\n",
    "# smokers over 60 is the same as the proportion of smokers under 60)\n",
    "c['population_fraction'] = functools.reduce(lambda x, y: x * y, fractions)\n",
    "# We assume the participants are drawn uniformly from the population.\n",
    "c['participant_fraction'] = c['population_fraction']\n",
    "# Assume some boosted incidence risk for subpopulations. We pick random numbers\n",
    "# here, but in actual use you'd put your best estimate for the incidence risk\n",
    "# of each demographic category.\n",
    "# Since we assume participants are uniformly drawn from the county population,\n",
    "# this actually doesn't end up affecting the estimated number of clinical events.\n",
    "c['incidence_scaler'] = functools.reduce(lambda x, y: x * y,\n",
    "                                         incidence_scalers)\n",
    "c.incidence_scaler.loc[dict(age='over_60')] = 1 + 2 * np.random.random()\n",
    "c.incidence_scaler.loc[dict(comorbidity=['smokers', 'diabetes', 'obese'])] = 1 + 2 * np.random.random()\n",
    "c.incidence_scaler.loc[dict(ethnicity=['black', 'hisp_lat'])] = 1 + 2 * np.random.random()\n",
    "\n",
    "# We assume a constant incidence_to_event_factor.\n",
    "c['incidence_to_event_factor'] = 0.6 * xr.ones_like(c.incidence_scaler)\n",
    "\n",
    "util.add_empty_history(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load incidence forecasts\n",
    "\n",
    "We load historical incidence data from [COVID-19 Open Data](https://github.com/GoogleCloudPlatform/covid-19-open-data) and forecasts from [COVID-19 Forecast Hub](https://github.com/reichlab/covid19-forecast-hub).\n",
    "\n",
    "We note that there are a set of caveats when using the CDC models that should be considered when using these for trial planning:\n",
    "* Forecasts are only available for US counties. Hence, these forecasts will only work for  US-only trials. Trials with sites outside the US will need to supplement these forecasts.\n",
    "* Forecasts only go out for four weeks. Trials take much longer than four weeks to complete, when measured from site selection to logging the required number of cases in the control arm. For simplicity, here we extrapolate incidence as *constant* after the last point of the forecast. Here we extrapolate out to March 1, 2021. \n",
    "* The forecasts from the CDC are provided with quantile estimates. Our method depends on getting *representative forecasts* from the model: we need a set of sample forecasts for each site which represent the set of scenarios that can occur. Ideally these scenarios will be equally probable so that we can compute probabilities by averaging over samples. To get samples from quantiles,   we interpolate/extrapolate  to get 100 evenly spaced quantile estimates, which we treat as representative samples.\n",
    "\n",
    "You can of course replace these forecasts with whatever represents your beliefs and uncertainty about what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolate out a bit extra to ensure we're within bounds when we interpolate later.\n",
    "full_pred = public_data.fetch_cdc_forecasts([('COVIDhub-ensemble', '2020-10-12'),\n",
    "                                             ('COVIDhub-baseline', '2020-10-12')],\n",
    "                                            end_date=c.time.values[-1] + np.timedelta64(15, 'D'),\n",
    "                                            num_samples=50)\n",
    "full_gt = public_data.fetch_opencovid_incidence()\n",
    "\n",
    "# Include more historical incidence here for context. It will be trimmed off when\n",
    "# we construct scenarios to simulate. The funny backwards range is to ensure that if\n",
    "# we use weekly instead of daily resolution, we use the same day of the week as c.\n",
    "time = np.arange(c.time.values[-1], np.datetime64('2020-05-01'), -time_resolution)[::-1]\n",
    "incidence_model = public_data.assemble_forecast(full_gt, full_pred, site_df, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = np.random.choice(c.location.values, size=5, replace=False)\n",
    "incidence_model.sel(location=locs).plot.line(x='time', color='k', alpha=.1, add_legend=False, col='location', row='model')\n",
    "plt.ylim(0.0, 1e-3)\n",
    "plt.suptitle('Forecast incidence at a sampling of sites', y=1.0)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simulate the trial\n",
    "\n",
    "Now that we've specified how the trial works, we can compute how the trial will turn out given the incidence forecasts you've specified. We do this by first imagining what sampling what incidence will be at all locations simultaneously. For any given fully-specified scenario, we compute how many participants will be under observation at any given time in any given location (in any given combination of demographic buckets), then based on the specified local incidence we compute how many will become infected, and how many will produce clinical events.\n",
    "\n",
    "Here we assume that the incidence trajectories of different locations are drawn at random from the available forecasts. Other scenario-generation methods in `sim_scenarios` support more complex approaches. For example, we may be highly uncertain about the incidence at each site, but believe that if incidence is high at a site, then it will also be high at geographically nearby sites. If this is the case then the simulation should not choose forecasts independently at each site but instead should take these correlations into account.  The code scenario-generating methods in `sim_scenarios` allows us to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incidence_flattened: rolls together all the models you've included in your ensemble, treating them as independent samples.\n",
    "incidence_flattened = sim_scenarios.get_incidence_flattened(incidence_model, c)\n",
    "\n",
    "# incidence_scenarios: chooses scenarios given the incidence curves and your chosen method of scenario-generation.\n",
    "incidence_scenarios = sim_scenarios.generate_scenarios_independently(incidence_flattened, num_scenarios=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of participants recruited under your trial rule\n",
    "participants = sim.recruitment(c)\n",
    "# compute the number of control arm events under your trial rules and incidence_scenarios.\n",
    "events = sim.control_arm_events(c, participants, incidence_scenarios)\n",
    "\n",
    "plot_participants(participants)\n",
    "# plot events and label different vaccine efficacies\n",
    "plot_events(events)\n",
    "# plot histograms of time to success\n",
    "plot_success(c, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.add_stuff_to_ville(c, incidence_model, site_df, num_scenarios=100)\n",
    "metis_io.write_ville_to_netcdf(c, 'demo_data/site_list1_all_site_on.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimize the trial\n",
    "\n",
    "The simulations above supposed that all sites are activated as soon as possible (i.e. `site_activation` is identically 1). Now that we have shown the ability to simulate the outcome of the trial, we can turn it into a mathematical optimization problem. \n",
    "\n",
    "**Given the parameters of the trial within our control, how can we set those parameters to make the trial most likely to succeed or to succeed as quickly as possible?**\n",
    "\n",
    "We imagine the main levers of control are which sites to activate or which sites to prioritize activating, and this is what is implemented here. \n",
    "\n",
    "However, the framework we have developed is very general and could be extended to just about anything you control which you can predict the impact of. For example,\n",
    "* If you can estimate the impact of money spent boosting recruitment of high-risk participants, we could use those estimates to help figure out how to best allocate a fixed budget.\n",
    "* If you had requirements for the number of people infected in different demographic groups, we could use those to help figure out how to best allocate doses between sites with different population characteristics.\n",
    "\n",
    "The optimization algorithms are implemented in [JAX](https://github.com/google/jax), a  python library that makes it possible to differentiate through native python and numpy functions. The flexibility of the language makes it possible to compose a variety of trial optimization scenarios and then to write algorithms that find optima.  There are a number of technical details in how the optimization algorithms are written that will be discussed elsewhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example: Optimizing Static site activations\n",
    "\n",
    "Suppose that the only variable we can control is which sites should be activated, and we have to make this decision at the beginning of the trial.  This decision is then set in stone for the duration of the trial.  To calculate this we proceed as follows:\n",
    "\n",
    "The optimizer takes in the trial plan, encoded in the xarray `c` as well as the `incidence_scenarios`, and then calls the optimizer to find the sites that should be activated to minimize the time to success of the trial. The algorithm modifies `c` *in place*, so that after the algorithm runs, it returns the trial plan `c` but with the site activations chosen to be on or off in accordance with the optimizion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time optimization.optimize_static_activation(c, incidence_scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the resulting sites\n",
    "\n",
    "Now we can plot the activations for the resulting sites. Only a fraction (92/146) of the original sites are activated in the optimized plan. If you compare the distributions for the time to success for the optimized sites to those in the original trial plan, where all sites were activated equally, you will see that the optimized plan saves time for the trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites = c.location.values\n",
    "activated_sites = c.location.values[c.site_activation.mean('time') == 1]\n",
    "\n",
    "# Simulate the results with this activation scheme.\n",
    "print(f'\\n\\n{len(activated_sites)} of {len(all_sites)} activated')\n",
    "participants = sim.recruitment(c)\n",
    "events = sim.control_arm_events(c, participants, incidence_scenarios)\n",
    "plot_participants(participants)\n",
    "plot_events(events)\n",
    "plot_success(c, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.add_stuff_to_ville(c, incidence_model, site_df, num_scenarios=100)\n",
    "metis_io.write_ville_to_netcdf(c, 'demo_data/site_list1_optimized_static.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example: prioritizing sites\n",
    "Suppose we can activate up to 20 sites each week for 10 weeks. How do we prioritize them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put all sites in on group. We also support prioritizing sites within groupings.\n",
    "# For example, if you can activate 2 sites per state per week, sites would be grouped\n",
    "# according to the state they're in.\n",
    "site_to_group = pd.Series(['all_sites'] * len(site_df), index=site_df.index)\n",
    "decision_dates = c.time.values[:70:7]\n",
    "allowed_activations = pd.DataFrame([[20] * len(decision_dates)], index=['all_sites'], columns=decision_dates)\n",
    "parameterizer = optimization.PivotTableActivation(c, site_to_group, allowed_activations, can_deactivate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization.optimize_params(c, incidence_scenarios, parameterizer)\n",
    "c['site_activation'] = c.site_activation.round()  # each site has to be on or off at each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = c.site_activation.to_pandas()\n",
    "df.columns = [pd.to_datetime(x).date() for x in df.columns]\n",
    "sns.heatmap(df, cbar=False)\n",
    "plt.title('Which sites are activated when')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = sim.recruitment(c)\n",
    "events = sim.control_arm_events(c, participants, incidence_scenarios)\n",
    "plot_participants(participants)\n",
    "plot_events(events)\n",
    "plot_success(c, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.add_stuff_to_ville(c, incidence_model, site_df, num_scenarios=100)\n",
    "metis_io.write_ville_to_netcdf(c, 'demo_data/site_list1_prioritized.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
